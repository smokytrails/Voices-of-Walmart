{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2a192f32-d68e-44ee-a9d7-7e7fd75320ef",
   "metadata": {},
   "source": [
    "# Employee Review Analysis\n",
    "## DATA200 Project by Harshita Nagasubramanian and Vishal Srikanth\n",
    "\n",
    "### Introduction\n",
    "This project is an exploratory analysis of Walmart Employee reviews and aims to find the general consensus of Walmart's work environment. When analyzing employee reviews, taken into consideration are the employment status of a Walmert employee (former/current), the location of the store (specificity can vary), a numerical rating (1-5), a written review and the date at which the review was posted. \n",
    "\n",
    "\n",
    "All the information is sourced from publicly available data. \n",
    "\n",
    "\n",
    "### Research Question\n",
    "\n",
    "\n",
    "### Additional Inference\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54d27964",
   "metadata": {},
   "source": [
    "### Importing the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "833f1dc7-f054-400c-9bf8-91f57f75f14d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install nltk matplotlib wordcloud\n",
    "#%pip install PIL\n",
    "%pip install pandas seaborn numpy scikit-learn\n",
    "#from PIL import Image\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from wordcloud import WordCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e02f5e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "reviews=pd.read_csv('walmart_cleaned.csv')\n",
    "reviews.head(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57474bdc",
   "metadata": {},
   "source": [
    "The following is a removal of a redundant column (index) and splitting of the Date column into Year, Month and Day columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79d3fe3e-2fed-4c0c-a873-b10b22ec1a82",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews=reviews.drop(columns=['index','date_posted'],axis=1)\n",
    "reviews.head(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3961f896",
   "metadata": {},
   "source": [
    "The following function is created to categorize the reviews into Pre-Covid, Covid and Post-Covid eras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6821952d-34b5-4b13-8b81-08d505308dd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def covid_categories(year):\n",
    "    if year<2020:\n",
    "        return 'pre-covid era'\n",
    "    elif 2020<=year<=2021:\n",
    "        return 'covid era'\n",
    "    elif year>2021:\n",
    "        return 'post-covid era'\n",
    "\n",
    "reviews['covid category']=reviews['year'].apply(covid_categories)\n",
    "reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3e648d8-ef23-4c40-b15b-17e979c8d0d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by the required columns and count the occurrences\n",
    "reviews_count = reviews.groupby(['rating', 'status', 'covid category']).size().reset_index(name='count')\n",
    "\n",
    "# Now you should have a 'count' column with the number of occurrences for each group\n",
    "reviews_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33e10081-d268-4370-a566-6de928deba1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the new colors for each Covid category and status\n",
    "covid_status_colors = {\n",
    "    'pre-covid era': {'Current Employee': 'darkgoldenrod', 'Former Employee': 'palegoldenrod'},\n",
    "    'covid era': {'Current Employee': 'midnightblue', 'Former Employee': 'cornflowerblue'},\n",
    "    'post-covid era': {'Current Employee': 'darkred', 'Former Employee': 'lightcoral'}\n",
    "}\n",
    "\n",
    "# Create a stacked bar chart\n",
    "fig, ax = plt.subplots(figsize=(14, 8))\n",
    "\n",
    "# Plot the bars\n",
    "width = 0.25  # the width of the bars\n",
    "ind = np.arange(len(reviews_count['rating'].unique()))  # the x locations for the groups\n",
    "\n",
    "# Group by 'covid_category' this time\n",
    "for i, (covid_category, sub_df) in enumerate(reviews_count.groupby('covid category')):\n",
    "    # Stack by 'status'\n",
    "    bottom = np.zeros(len(reviews['rating'].unique()))  # reset bottom for each covid category\n",
    "    for status in ['Current Employee', 'Former Employee']:\n",
    "        values = sub_df[sub_df['status'] == status]['count'].values\n",
    "        ax.bar(ind + i * width, values, width, bottom=bottom, color=covid_status_colors[covid_category][status], label=f\"{covid_category} ({status})\" if bottom.sum() == 0 else \"\")\n",
    "        bottom += values\n",
    "\n",
    "# Set the chart title and labels\n",
    "ax.set_title('Employee Ratings by Covid Era and Employment Status', fontsize=16)\n",
    "ax.set_xlabel('Rating', fontsize=14)\n",
    "ax.set_ylabel('Count', fontsize=14)\n",
    "\n",
    "# Set the x-ticks and x-tick labels\n",
    "ax.set_xticks(ind + width)\n",
    "ax.set_xticklabels(('1', '2', '3', '4', '5'))\n",
    "\n",
    "# Add a legend\n",
    "ax.legend(title='Covid Category (Status)')\n",
    "\n",
    "# Show the plot\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3ead7ba",
   "metadata": {},
   "source": [
    "The following is done to make the RegEx analysis easier, all the state names are normalized to only show state abbreviations. In the dataset, some of the locations have state abbreviations and state names, and are inconsistent with their capitalizations. Additionally, reviews with no state specified are removed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46c7d6ec-d133-4c08-a73f-163fea385441",
   "metadata": {},
   "outputs": [],
   "source": [
    "states = [\n",
    "    ['Alabama', 'AL'], ['Alaska', 'AK'], ['Arizona', 'AZ'], ['Arkansas', 'AR'], ['California', 'CA'],\n",
    "    ['Colorado', 'CO'], ['Connecticut', 'CT'], ['Delaware', 'DE'], ['Florida', 'FL'], ['Georgia', 'GA'],\n",
    "    ['Hawaii', 'HI'], ['Idaho', 'ID'], ['Illinois', 'IL'], ['Indiana', 'IN'], ['Iowa', 'IA'],\n",
    "    ['Kansas', 'KS'], ['Kentucky', 'KY'], ['Louisiana', 'LA'], ['Maine', 'ME'], ['Maryland', 'MD'],\n",
    "    ['Massachusetts', 'MA'], ['Michigan', 'MI'], ['Minnesota', 'MN'], ['Mississippi', 'MS'], ['Missouri', 'MO'],\n",
    "    ['Montana', 'MT'], ['Nebraska', 'NE'], ['Nevada', 'NV'], ['New Hampshire', 'NH'], ['New Jersey', 'NJ'],\n",
    "    ['New Mexico', 'NM'], ['New York', 'NY'], ['North Carolina', 'NC'], ['North Dakota', 'ND'], ['Ohio', 'OH'],\n",
    "    ['Oklahoma', 'OK'], ['Oregon', 'OR'], ['Pennsylvania', 'PA'], ['Rhode Island', 'RI'], ['South Carolina', 'SC'],\n",
    "    ['South Dakota', 'SD'], ['Tennessee', 'TN'], ['Texas', 'TX'], ['Utah', 'UT'], ['Vermont', 'VT'],\n",
    "    ['Virginia', 'VA'], ['Washington', 'WA'], ['West Virginia', 'WV'], ['Wisconsin', 'WI'], ['Wyoming', 'WY']\n",
    "]\n",
    "\n",
    "def state_column(location):\n",
    "    for state in states:\n",
    "        if state[0].lower() in location.lower() or state[1].lower() in location.lower():\n",
    "            return state[1]\n",
    "    return 'none'\n",
    "\n",
    "reviews['state']=reviews['location'].apply(state_column)\n",
    "reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50205579-3059-43b9-b69d-134876bed55d",
   "metadata": {},
   "outputs": [],
   "source": [
    "none_states = reviews[reviews['state'] == 'none']\n",
    "none_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "068066cb-ca5d-4d32-b1cc-b263e0068ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = reviews[reviews['state'] != 'none']\n",
    "reviews=reviews.drop(columns=['location'], axis=1)\n",
    "reviews.reset_index(drop=True, inplace=True)\n",
    "reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "099ef037",
   "metadata": {},
   "source": [
    "Next, we take the average rating per state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc55ff34-d84e-436f-acaf-d199b8b1685d",
   "metadata": {},
   "outputs": [],
   "source": [
    "average_rating_by_state = reviews.groupby('state')['rating'].mean().reset_index()\n",
    "\n",
    "# Rename the columns to match your desired new dataset\n",
    "average_rating_by_state.columns = ['state', 'average rating']\n",
    "\n",
    "# Print or display the new dataset\n",
    "average_rating_by_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c839d86-8297-4138-b024-74e915af27e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install geopandas\n",
    "import geopandas as gpd\n",
    "US = gpd.read_file(\"gz_2010_us_040_00_5m.json\")\n",
    "US"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f61c820f-d22b-4cd1-9fd9-851ff3ca68a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the list of states to be removed\n",
    "states_to_remove = ['District of Columbia', 'Puerto Rico', 'Alaska', 'Hawaii']  # Add other states as needed\n",
    "\n",
    "# Function to remove states from the GeoDataFrame\n",
    "def remove_states(row):\n",
    "    if row['NAME'] in states_to_remove:\n",
    "        return None\n",
    "    else:\n",
    "        return row\n",
    "\n",
    "# Apply the function to filter out specified states\n",
    "US_filtered = US.apply(remove_states, axis=1).dropna()\n",
    "US_filtered['STATE ABB']=US_filtered['NAME'].apply(state_column)\n",
    "US_filtered.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Print the filtered GeoDataFrame\n",
    "US_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acb6158b-53f4-4a23-8103-407fed5d1d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the shapefile data with the average rating data\n",
    "merged_data = US_filtered.merge(average_rating_by_state, how='left', left_on='STATE ABB', right_on='state')\n",
    "\n",
    "# Increase the figure size here\n",
    "fig, ax = plt.subplots(1, figsize=(20, 15))  # You can increase these numbers\n",
    "\n",
    "# Plot the choropleth map\n",
    "merged_data.plot(column='average rating', cmap='viridis', linewidth=0.8, ax=ax, edgecolor='0.8', legend=True)\n",
    "\n",
    "# Set plot title\n",
    "plt.title('Average Rating by State')\n",
    "\n",
    "for idx, row in US_filtered.iterrows():\n",
    "    # Get the centroid of the polygon\n",
    "    centroid = row['geometry'].centroid\n",
    "    # Add the state name as text at the centroid\n",
    "    ax.text(centroid.x, centroid.y, row['STATE ABB'], fontsize=8, ha='center', color='black')\n",
    "\n",
    "# Remove the axis\n",
    "ax.axis('off')\n",
    "\n",
    "# Optional: Adjust the aspect ratio\n",
    "ax.set_aspect('equal')\n",
    "\n",
    "# Optional: Adjust the limits if the map is not filling the figure\n",
    "plt.xlim(-130, -65)  # Adjust these values as needed to fit your desired area\n",
    "plt.ylim(20, 50)     # Adjust these values as needed to fit your desired area\n",
    "\n",
    "# Optional: Adjust layout\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be6e7bec-3a15-49b7-ab02-4947727f9cee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate average rating for each state and year combination\n",
    "average_rating_by_state_year = reviews.groupby(['state', 'year'])['rating'].mean().reset_index()\n",
    "\n",
    "# Plot actual and predicted average rating against year for each state\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Input state\n",
    "state=input('Enter state: ')\n",
    "# Filter data for the current state\n",
    "data = average_rating_by_state_year[average_rating_by_state_year['state'] == state]\n",
    "    \n",
    "# Prepare the data for linear regression\n",
    "X = data[['year']]\n",
    "y = data['rating']\n",
    "    \n",
    "# Initialize the linear regression model\n",
    "model = LinearRegression()\n",
    "    \n",
    "# Fit the model\n",
    "model.fit(X, y)\n",
    "    \n",
    "# Make predictions\n",
    "y_pred = model.predict(X)\n",
    "    \n",
    "# Plot actual ratings\n",
    "plt.scatter(data['year'], data['rating'], label=f'Actual {state} Ratings')\n",
    "    \n",
    "# Plot predicted ratings\n",
    "plt.plot(data['year'], y_pred, label=f'Predicted {state} Ratings', linestyle='--')\n",
    "\n",
    "# Labels and title\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Average Rating')\n",
    "plt.title('Actual vs Predicted Average Rating by State Over Time')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c697e1f-00e5-45d1-892d-7e9b6f08e6ff",
   "metadata": {},
   "source": [
    "The inference task for the described bootstrapping analysis involves understanding the variability and uncertainty associated with certain parameters or model predictions based on a given dataset.\n",
    "\n",
    "In the provided code, we perform two main inference tasks using bootstrapping:\n",
    "\n",
    "Parameter Estimation: We estimate the mean rating of the dataset using bootstrapping. By repeatedly resampling the dataset with replacement and calculating the mean rating for each resample, we obtain a distribution of mean ratings. From this distribution, we can calculate confidence intervals to understand the range of plausible values for the mean rating. This allows us to infer the uncertainty associated with our estimate of the mean rating.\n",
    "\n",
    "Model Uncertainty: We assess the uncertainty of a linear regression model's predictions using bootstrapping. By resampling the dataset with replacement, fitting a linear regression model to each resample, and calculating the Mean Squared Error (MSE) for each model, we obtain a distribution of MSE scores. From this distribution, we can calculate confidence intervals to understand the variability in model performance. This allows us to infer the uncertainty associated with the model's predictions.\n",
    "\n",
    "In summary, the inference task is to quantify and understand the uncertainty and variability associated with certain parameters (mean rating) and model predictions (MSE) based on the given dataset through bootstrapping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01c66744-2d58-4da4-a275-d9e8de66db65",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Function to perform bootstrapping and estimate parameter\n",
    "def bootstrap_parameter_estimation(data, parameter='rating', n_bootstraps=1000):\n",
    "    parameters = []\n",
    "    for _ in range(n_bootstraps):\n",
    "        # Resample data with replacement\n",
    "        resample = data.sample(n=len(data), replace=True)\n",
    "        # Calculate parameter of interest (mean rating in this case)\n",
    "        parameter_value = resample[parameter].mean()\n",
    "        parameters.append(parameter_value)\n",
    "    return parameters\n",
    "\n",
    "# Perform bootstrapping for parameter estimation\n",
    "bootstrapped_ratings = bootstrap_parameter_estimation(reviews)\n",
    "\n",
    "# Calculate confidence interval\n",
    "confidence_interval = np.percentile(bootstrapped_ratings, [2.5, 97.5])\n",
    "print(\"95% Confidence Interval for Mean Rating:\", confidence_interval)\n",
    "\n",
    "# Function to perform bootstrapping for model uncertainty\n",
    "def bootstrap_model_uncertainty(data, n_bootstraps=100):\n",
    "    mse_scores = []\n",
    "    for _ in range(n_bootstraps):\n",
    "        # Resample data with replacement\n",
    "        resample = data.sample(n=len(data), replace=True)\n",
    "        # Fit a linear regression model\n",
    "        X_train = resample[['year']]\n",
    "        y_train = resample['rating']\n",
    "        X_test = data[['year']]  # You may want to use a separate test set\n",
    "        y_test = data['rating']   # You may want to use a separate test set\n",
    "        model = LinearRegression()\n",
    "        model.fit(X_train, y_train)\n",
    "        # Make predictions\n",
    "        y_pred = model.predict(X_test)\n",
    "        # Calculate Mean Squared Error\n",
    "        mse = mean_squared_error(y_test, y_pred)\n",
    "        mse_scores.append(mse)\n",
    "    return mse_scores\n",
    "\n",
    "# Perform bootstrapping for model uncertainty\n",
    "bootstrapped_mse = bootstrap_model_uncertainty(reviews)\n",
    "\n",
    "# Calculate confidence interval\n",
    "confidence_interval_mse = np.percentile(bootstrapped_mse, [2.5, 97.5])\n",
    "print(\"95% Confidence Interval for Model Uncertainty (MSE):\", confidence_interval_mse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ac69d28-6a2b-4ac0-b0e2-0e40fcbf13d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot bootstrapped mean ratings\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(bootstrapped_ratings, bins=30, color='skyblue', edgecolor='black', alpha=0.7)\n",
    "plt.xlabel('Mean Rating')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Bootstrapped Mean Ratings Distribution')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Plot bootstrapped model uncertainties (MSE)\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(bootstrapped_mse, bins=30, color='salmon', edgecolor='black', alpha=0.7)\n",
    "plt.xlabel('Model Uncertainty (MSE)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Bootstrapped Model Uncertainty (MSE) Distribution')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4501ce65-e314-4fc2-a549-1ea7601b10b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Assuming 'reviews' dataset is defined somewhere in your code\n",
    "\n",
    "# Prepare features and target variable\n",
    "X = reviews[['year']]  # Feature\n",
    "y = reviews['rating']  # Target variable\n",
    "\n",
    "# Initialize models\n",
    "linear_regression_model = LinearRegression()\n",
    "random_forest_model = RandomForestRegressor()\n",
    "\n",
    "# Hyperparameter tuning using cross-validation\n",
    "# Linear Regression\n",
    "linear_regression_scores = cross_val_score(linear_regression_model, X, y, cv=5, scoring='neg_mean_squared_error')\n",
    "linear_regression_rmse = np.sqrt(-linear_regression_scores.mean())\n",
    "\n",
    "# Random Forest\n",
    "random_forest_scores = cross_val_score(random_forest_model, X, y, cv=5, scoring='neg_mean_squared_error')\n",
    "random_forest_rmse = np.sqrt(-random_forest_scores.mean())\n",
    "\n",
    "# Model comparison\n",
    "print(\"Linear Regression RMSE:\", linear_regression_rmse)\n",
    "print(\"Random Forest RMSE:\", random_forest_rmse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b158bf2-687d-4ec0-a23c-fe4f2eb347b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the number of folds for cross-validation\n",
    "n_folds = 5\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Plot RMSE for each fold and model (Linear Regression and Random Forest)\n",
    "plt.plot(range(1, n_folds + 1), np.sqrt(-linear_regression_scores), marker='o', label='Linear Regression')\n",
    "plt.plot(range(1, n_folds + 1), np.sqrt(-random_forest_scores), marker='o', label='Random Forest')\n",
    "\n",
    "# Labels and title\n",
    "plt.xlabel('Fold')\n",
    "plt.ylabel('RMSE')\n",
    "plt.title('Cross-Validation Results')\n",
    "plt.xticks(range(1, n_folds + 1))\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
